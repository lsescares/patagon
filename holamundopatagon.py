# -*- coding: utf-8 -*-
"""holamundopatagon.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NOFGUNKSsrdXHYBr51BLHgtQhS-7BUoS

# 3.1 Dataset
En este problema se dispone de una serie de puntos (dataset) en el espacio cartesiano $(x_1, x_2)$, los cuales son clasificados en base a su posición en el espacio. Lea con atención el código a continuación que permite la generación de este dataset `(X, Y)`.
"""

# generación dataset------

# importar librerías
import random
import numpy as np
import matplotlib.pyplot as plt
from math import cos, sin, pi

# cantidad de datos en el dataset
data_size = 5000

# proporción de ruido en el dataset
#data_noise = 0.0

#-------------------------------------
#Agregando ruido a los datos
data_noise=0.1
#data_noise=0.25
#data_noise=0.45
#--------------------------------------
# inicializar sets de datos
# X contiene los puntos cartesianos (x, y)
# Y contiene las etiquetas de clasificación de cada punto
# donde 1 significa que se encuentra dentro del radio y 0 que no.
X = np.zeros( (data_size, 2) )
Y = np.zeros( (data_size, 2) )

# generar datos al azar
for i in np.arange(data_size):
  x1 = 2*random.random() - 1
  x2 = 2*random.random() - 1

  # agregar al set X de puntos
  X[i, 0] = x1
  X[i, 1] = x2

  # agregar etiqueta al set Y
  if cos(x1*3*pi)*cos(x2*3*pi) > 0.0:
    Y[i, 0] = 0
    Y[i, 1] = 1
  else:
    Y[i, 0] = 1
    Y[i, 1] = 0

  # agregar ruido al dato (utilizado más adelante)
  if random.random() < data_noise:
    Y[i, :] = 1 - Y[i, :]

# plotear distribución en el dataset
plt.figure( figsize=(7,7) )
plt.scatter(X[:,0], X[:,1], c=Y[:,0], cmap='plasma', alpha=0.5, s=50)

"""# 3.2 Data Splitting

"""

# importar librerías
from sklearn.model_selection import train_test_split
#from keras.utils import to_categorical

# generar sets de datos de training y testing
# la varibale test_size permite controlar la proporción entre los datos de testing y training.
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state = 0)

"""## 3.3 Model Setup"""

# importar librerías
import keras 
from keras.models import Sequential
from keras.layers import Input
from keras.layers import Dense
from keras.layers import Dropout
from keras.utils import np_utils
#from keras.utils import plot_model

# inicializar modelo keras.Sequential
model = Sequential()

# ---
# primero debemos agregar nuestra capa Input donde debemos especificar
# las dimensiones de los datos que se ingresarán al modelo
input_dim = ( 2, )
model.add( Input( shape=input_dim ) )

# ---
# ahora debemos ir agregando nuestras capas Dense.
# https://keras.io/api/layers/core_layers/dense/

# las keras.layers.Dense reciben la cantidad de nodos o units dentro
# de la capa y la función de activación que operarán.
# https://keras.io/api/layers/activations/

'''
#agregando mas capas
model.add(Dense(units = 12, activation = 'relu'))
model.add(Dense(units = 12, activation = 'relu'))
model.add(Dense(units = 12, activation = 'relu'))#<-- con 3 hidden layers
model.add(Dense(units = 12, activation = 'relu'))#<-- con 4 hidden layers
model.add(Dense(units = 12, activation = 'relu'))#<-- con 5 hidden layers
model.add(Dense(units = 12, activation = 'relu'))#<-- con 6 hidden layers
'''

'''
#Aumentando a 60 el numero de nodos
model.add(Dense(units = 60, activation = 'relu'))
model.add(Dense(units = 60, activation = 'relu'))
model.add(Dense(units = 60, activation = 'relu'))
'''

#'''
#Aumentando a 120 el numero de nodos
model.add(Dense(units = 240, activation = 'relu'))
model.add(Dense(units = 240, activation = 'relu'))
model.add(Dense(units = 240, activation = 'relu'))
#'''

'''
#cambiando funcion de activacion a tanh
model.add(Dense(units = 240, activation = 'tanh'))
model.add(Dense(units = 240, activation = 'tanh'))
model.add(Dense(units = 240, activation = 'tanh'))
'''

'''
#cambiando funcion de activacion a sigmoid
model.add(Dense(units = 240, activation = 'exponential'))
model.add(Dense(units = 240, activation = 'exponential'))
model.add(Dense(units = 240, activation = 'exponential'))
'''

# ---
# por último debemos configurar nuestra capa de salida
# dado que el modelo consiste en uno de clasificación emplearemos
# la función softmax, donde cada nodo indicará la probabilidad de que
# los datos correspondan a una de las etiquetas o estados de salud.
labels_num = 2
model.add(Dense(units = labels_num, activation = 'softmax'))

# imprimir resumen del modelo
model.summary()

"""#Compilacion y entrenamiento"""

# ---
# compilar modelo siguiendo como función de pérdida
# la categorical crossentropy, 'categorical_crossentropy'
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

# ---
# realizar rutina de entrenamiento
train_history = model.fit(X_train, Y_train,
                          batch_size=128, epochs=200,
                          validation_data=(X_test, Y_test) )

